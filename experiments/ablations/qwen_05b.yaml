# Qwen2.5-0.5B Ablation Configuration
# Purpose: Code model transfer validation (PRD Section 3.3)
# Hardware: RTX 4090 (24GB VRAM)

experiment:
  name: "qwen25-05b-ablation"
  description: "Qwen2.5-0.5B ablation for code model transfer validation"
  tags: ["ablation", "code-model", "transfer", "qwen"]

# Model Configuration
model:
  name: "Qwen/Qwen2.5-0.5B"
  revision: "main"
  dtype: "bfloat16"
  max_seq_length: 2048
  trust_remote_code: true

# LoRA Configuration
lora:
  enabled: true
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Training Stages
stages:
  sft:
    enabled: true
    dataset: "output/processed/think/sft"
    num_epochs: 2
    learning_rate: 2e-5
    warmup_ratio: 0.03
    weight_decay: 0.1
    per_device_batch_size: 1
    gradient_accumulation_steps: 32
    effective_batch_size: 32
    max_seq_length: 32768

  dpo:
    enabled: true
    dataset: "output/processed/think/dpo"
    beta: 0.1
    learning_rate: 5e-7
    num_epochs: 1
    warmup_ratio: 0.1
    per_device_batch_size: 1
    gradient_accumulation_steps: 8
    max_length: 2048

  rlvr:
    enabled: true
    dataset: "output/processed/think/rl"
    num_generations: 4
    learning_rate: 1e-6
    num_epochs: 1

# RTX 4090 Optimizations
hardware:
  gpu: "RTX 4090"
  vram_gb: 24
  use_flash_attention: true
  use_gradient_checkpointing: true
  mixed_precision: "bf16"

# Evaluation
evaluation:
  dataset: "smartbugs"
  eval_every_n_steps: 500
  metrics:
    - "precision"
    - "recall"
    - "f1"
    - "false_positive_rate"
  primary_metric: "f1"
  target_f1: 0.25  # Higher target for code-pretrained model

# Experiment Tracking
wandb:
  project: "smart-contract-vuln-detection"
  group: "ablations-qwen-05b"
  tags: ["qwen", "0.5b", "ablation", "code-transfer"]

# Output
output:
  dir: "checkpoints/qwen25-05b"
  save_total_limit: 3
  push_to_hub: false
