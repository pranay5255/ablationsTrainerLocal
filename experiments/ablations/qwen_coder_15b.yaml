# Qwen2.5-Coder-1.5B Ablation Configuration
# Purpose: Final pre-scale validation (PRD Section 3.3)
# Hardware: RTX 4090 (24GB VRAM)

experiment:
  name: "qwen25-coder-15b-ablation"
  description: "Qwen2.5-Coder-1.5B ablation for final pre-scale validation before 3B"
  tags: ["ablation", "pre-scale", "qwen-coder", "1.5b"]

# Model Configuration
model:
  name: "Qwen/Qwen2.5-Coder-1.5B"
  revision: "main"
  dtype: "bfloat16"
  max_seq_length: 2048
  trust_remote_code: true
  # 4-bit quantization for memory efficiency on RTX 4090
  load_in_4bit: false  # Set true if OOM

# LoRA Configuration (reduced rank for larger model)
lora:
  enabled: true
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Training Stages
stages:
  cpt:
    enabled: false  # Skip CPT for pre-scale validation
    tokens: 10_000_000_000  # 10B tokens as per PRD for 1.5B

  sft:
    enabled: true
    dataset: "output/processed/sft"
    num_epochs: 2
    learning_rate: 2e-5
    warmup_ratio: 0.0
    weight_decay: 0.01
    per_device_batch_size: 2
    gradient_accumulation_steps: 16
    effective_batch_size: 32
    save_steps: 500
    eval_steps: 500
    logging_steps: 50

  dpo:
    enabled: true
    dataset: "output/processed/dpo"
    beta: 0.1
    learning_rate: 5e-7
    num_epochs: 1
    warmup_ratio: 0.1
    per_device_batch_size: 1
    gradient_accumulation_steps: 16
    max_length: 2048
    max_prompt_length: 1024
    save_steps: 200
    eval_steps: 200

  grpo:
    enabled: false
    num_generations: 4
    learning_rate: 1e-6
    num_epochs: 1

# RTX 4090 Optimizations (critical for 1.5B)
hardware:
  gpu: "RTX 4090"
  vram_gb: 24
  use_flash_attention: true
  use_gradient_checkpointing: true  # Required for 1.5B
  mixed_precision: "bf16"

# Evaluation
evaluation:
  dataset: "smartbugs"
  eval_every_n_steps: 500
  metrics:
    - "precision"
    - "recall"
    - "f1"
    - "false_positive_rate"
  primary_metric: "f1"
  target_f1: 0.35  # Day 40 checkpoint

# Experiment Tracking
wandb:
  project: "smart-contract-vuln-detection"
  group: "ablations-qwen-coder-15b"
  tags: ["qwen-coder", "1.5b", "ablation", "pre-scale"]

# Output
output:
  dir: "checkpoints/qwen25-coder-15b"
  save_total_limit: 3
  push_to_hub: false
